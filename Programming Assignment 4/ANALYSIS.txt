A description of the (recursive) formula used to calculate the optimal costs
OPERATION(i,j) is the optimal cost of reaching from depature point i to departue point j
OPERATION(i,j) = minimum(over j <= k < i)(CostMatrix[i,k] + OPERATION(k,j))

base case is OPERATION(i,i) = 0 for all i from 1 to n

A theoretical analysis of the runtime of the algorithm
Dynamic Programming: O(n^3) as we have 3 nested loops
Memoization: O(n^2) as we avoid doing the same recursive calls multiple times

A comparison of the empirical timing results of both algorithms, and an explanation
of the observed results
Both algorithms were tested on the same test data, and the results are as follows:
Memoization: 2000 ns
Dynamic Programming: 900 ns
So here is where things get interesting. Running on my machine, with the given test data, the dynamic programming method is faster.
But its not that simple. Upon experimentation, I found that the numbers would fluctuate wildly based on the order in which I called the two algorithms.
Alongside that, if I "warm up" the JVM by calling the algorithm once before then recording the time, it affects the running time randomly again.
On my hardware atleast, it would seem like the dynamic programming method is faster, even though it should actually be slower.
My hypothesis on why this is, is something to due with CPU caching or JVM caching, wherin since I am not using full recursion for my dynamic programming method, the JVM is able to be more efficient due to not needing to allocate as much memory to active recursive calls.

One interesting part to look at, is the ammount of inner loop actions happening
Memoization Count: 6
Dynamic Count: 10
As we can see, memoization is doing less work than dynamic programming as far as the number of inner loop actions go.
This is due to the lookup based nature of the memoization approach
